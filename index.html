<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning Layers Overview</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <h1>Deep Learning Layers Overview</h1>

        <div class="section" id="dense-layer">
            <h2>1. Dense (Fully Connected) Layer</h2>
            <p><strong>Description:</strong> Connects each neuron in one layer to every neuron in the next layer. Useful for combining features extracted by previous layers.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.Dense(units, activation=None)</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import Dense

model.add(Dense(units=128, activation='relu'))</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="conv-layer">
            <h2>2. Convolutional Layer</h2>
            <p><strong>Description:</strong> Applies convolutional filters to the input data to detect local patterns. Commonly used in image processing.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.Conv2D(filters, kernel_size, activation=None, input_shape=None)</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import Conv2D

model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)))</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="maxpool-layer">
            <h2>3. MaxPooling Layer</h2>
            <p><strong>Description:</strong> Reduces the spatial dimensions of the input by taking the maximum value over a defined window. Useful for down-sampling.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.MaxPooling2D(pool_size, strides=None, padding='valid')</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import MaxPooling2D

model.add(MaxPooling2D(pool_size=(2, 2)))</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="flatten-layer">
            <h2>4. Flatten Layer</h2>
            <p><strong>Description:</strong> Flattens the input into a one-dimensional vector, used before feeding into a Dense layer.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.Flatten()</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import Flatten

model.add(Flatten())</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="embedding-layer">
            <h2>5. Embedding Layer</h2>
            <p><strong>Description:</strong> Transforms categorical variables into dense vectors of fixed size. Used primarily in NLP tasks.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.Embedding(input_dim, output_dim, input_length)</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import Embedding

model.add(Embedding(input_dim=1000, output_dim=64, input_length=10))</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="attention-layer">
            <h2>6. Attention Layer</h2>
            <p><strong>Description:</strong> Computes a weighted sum of inputs, allowing the model to focus on important parts of the input sequence.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code># TensorFlow does not have a direct Attention layer; it's often implemented using custom layers or in higher-level APIs.</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import Add, Dot, Activation

# Example implementation (simplified)
query = tf.keras.layers.Input(shape=(None, 64))
key = tf.keras.layers.Input(shape=(None, 64))
value = tf.keras.layers.Input(shape=(None, 64))

attention = Dot(axes=-1)([query, key])
attention = Activation('softmax')(attention)
context = Dot(axes=-1)([attention, value])</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="residual-layer">
            <h2>7. Residual Layer</h2>
            <p><strong>Description:</strong> Facilitates the training of very deep networks by introducing shortcut connections that bypass one or more layers.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code># Implemented using functional API for custom residual blocks</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import Add, Conv2D, Input

input = Input(shape=(64, 64, 3))
x = Conv2D(32, (3, 3), padding='same')(input)
x = Conv2D(32, (3, 3), padding='same')(x)
x = Add()([x, input])  # Shortcut connection</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="global-pooling-layer">
            <h2>8. Global Average Pooling Layer</h2>
            <p><strong>Description:</strong> Reduces the spatial dimensions of feature maps by computing the average across spatial dimensions.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.GlobalAveragePooling2D()</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import GlobalAveragePooling2D

model.add(GlobalAveragePooling2D())</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="batch-norm-layer">
            <h2>9. Batch Normalization Layer</h2>
            <p><strong>Description:</strong> Normalizes the activations of a layer for each mini-batch. Helps to stabilize and accelerate training.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.BatchNormalization()</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import BatchNormalization

model.add(BatchNormalization())</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="dropout-layer">
            <h2>10. Dropout Layer</h2>
            <p><strong>Description:</strong> Randomly sets a fraction of input units to 0 during training to prevent overfitting.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.Dropout(rate)</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import Dropout

model.add(Dropout(rate=0.5))</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="rnn-layer">
            <h2>11. Recurrent Neural Network (RNN) Layer</h2>
            <p><strong>Description:</strong> Processes sequences by iterating through the elements and maintaining a hidden state. Useful for time series and NLP tasks.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.SimpleRNN(units, activation=None, return_sequences=False)</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import SimpleRNN

model.add(SimpleRNN(units=64, activation='tanh'))</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="lstm-layer">
            <h2>12. Long Short-Term Memory (LSTM) Layer</h2>
            <p><strong>Description:</strong> A type of RNN that uses gating mechanisms to better capture long-term dependencies in sequences.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.LSTM(units, activation=None, return_sequences=False)</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import LSTM

model.add(LSTM(units=128, activation='tanh'))</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>

        <div class="section" id="gru-layer">
            <h2>13. Gated Recurrent Unit (GRU) Layer</h2>
            <p><strong>Description:</strong> A simplified version of LSTM with fewer gates, making it computationally efficient while still capturing dependencies in sequences.</p>
            <p><strong>Syntax:</strong></p>
            <div class="code-editor">
                <pre><code>tf.keras.layers.GRU(units, activation=None, return_sequences=False)</code></pre>
            </div>
            <p><strong>Example Code:</strong></p>
            <div class="code-editor">
                <pre><code>from tensorflow.keras.layers import GRU

model.add(GRU(units=128, activation='tanh'))</code></pre>
            </div>
            <button class="run-btn" data-link="https://colab.research.google.com/drive/1H5-3IfX2FLyUS67H-uVtH_jNlFs4qB4R?usp=sharing">Run on Google Colab</button>
        </div>
    </div>
    <script src="scripts.js"></script>
</body>
</html>
